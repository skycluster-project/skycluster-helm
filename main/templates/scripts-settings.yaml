{{ if or .Values.install }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: script-kubemesh-setup
  namespace: skycluster-system
  labels:
    skycluster.io/config-type: kubemesh-setup
data:
  functions.sh: |
    #!/usr/bin/env bash
    
    # Helper to pull a key from the secret and write to a file (base64-decoded)
    _pull_secret_key () {
      local key="$1" out="$2"
      # Note: dot in key must be escaped in jsonpath
      local jsonpath="{.data.${key//./\\.}}"
      local b64
      b64="$(kubectl -n "${NAMESPACE}" get secret "${ROOT_SECRET_NAME}" -o jsonpath="${jsonpath}" 2>/dev/null || true)"
      if [ -n "${b64}" ]; then
        echo "${b64}" | base64 -d > "${out}"
        chmod 0600 "${out}" || true
        echo "----   wrote ${out}"
      else
        echo "----   WARN: key '${key}' missing in secret '${ROOT_SECRET_NAME}'"
      fi
    }

    # We need to keep track of the root CA status
    # create or patch configmap cacerts-status in skycluster-system namespace
    _helper_set_status() {
      local ns="$1"
      local name="$2"
      local cluster_name="$3"
      local result="$4"
      local ownerName="$5"
      local owner_patch="$6"

      # extract the owner uid
      owner_uid=$(echo ${owner_patch} | jq -e -r '.metadata.ownerReferences[0].uid')
      if [[ -z "${owner_uid}" ]]; then
        echo "WARNING: owner uid is empty or invalid; skipping owner reference"
        exit 1
      fi

      # truncate owner_uid to 8 characters for brevity
      owner_uid_short="${owner_uid:0:8}"
      
      # cm_name="$name-$(openssl rand -hex 3)"
      cm_name="$name-${owner_uid_short}"

      if ! kubectl get configmap "$cm_name" -n "$ns" >/dev/null 2>&1; then
        echo " --> Creating ConfigMap ${cm_name} in namespace ${ns}..."
        kubectl create configmap "$cm_name" -n "$ns" \
          --from-literal="${cluster_name}=${result}" --dry-run=client -o yaml \
          | kubectl label --local -f - --overwrite \
          "skycluster.io/managed-by=skycluster" \
          "skycluster.io/config-type=status-result" \
          "skycluster.io/result-type=${name}" \
          "skycluster.io/owner-uid=${owner_uid}" \
          --dry-run=client -o yaml \
          | kubectl patch --local -f - --type merge -p "${owner_patch}" -o yaml \
          | kubectl -n "$ns" apply -f -
      else
        echo "Patching ConfigMap ${cm_name} in namespace ${ns}..."
        kubectl patch configmap "$cm_name" -n "$ns" \
          --type merge \
          -p "$(printf '{"data":{"%s":"%s"}}' "$cluster_name" "$result")"

        # Ensure/merge OwnerReference (strategic merge uses uid as merge cluster_name)
        kubectl patch configmap "$cm_name" -n "$ns" \
          --type merge \
          -p "${owner_patch}"
      fi
      echo "Done with ${cm_name} in ${ns}."
    }

    setup_kubeconfig() {
      local kubeconfig_b64="$1"

      if [[ -z "${kubeconfig_b64}" ]]; then
          echo "ERROR: Base64 kubeconfig input is required" >&2
          echo "Usage: setup_kubeconfig <base64-kubeconfig>" >&2
          return 1
      fi

      # Decode base64 kubeconfig
      local kubeconfig
      kubeconfig=$(echo "${kubeconfig_b64}" | base64 -d 2>/dev/null)
      if [[ $? -ne 0 ]]; then
          echo "ERROR: Failed to decode base64 kubeconfig" >&2
          return 1
      fi

      # Create temp dir and save kubeconfig
      local tmpdir
      tmpdir="$(mktemp -d)"
      local kubeconfig_path="${tmpdir}/kubeconfig.yaml"
      echo "${kubeconfig}" > "${kubeconfig_path}"

      # Return the kubeconfig path
      echo "${kubeconfig_path}"
    }

    setup_istio() {
      local workdir="/work"
      local istio_ver="1.27.0"
      local istio_url="https://github.com/istio/istio/releases/download/${istio_ver}/istio-${istio_ver}-linux-amd64.tar.gz"

      local tarfile="istio-${istio_ver}-linux-amd64.tar.gz"

      mkdir -p "$workdir" && cd "$workdir" || return 1
      if ! curl -fsSL "${istio_url}" -o "${tarfile}"; then
          echo "ERROR: Failed to download Istio from ${istio_url}" >&2
          return 1
      fi

      if ! tar -xzf "${tarfile}"; then
          echo "ERROR: Failed to extract ${tarfile}" >&2
          return 1
      fi

      echo "${workdir}/istio-${istio_ver}"
    }

  istio-root-ca.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    # The script below creates 
    #   - istio root ca along with
    #   - istio local cluster CA certs
    #   - istio remote secret
    #
    # and store in three different secrets:
    #   - ROOT_SECRET_NAME for root ca
    #   - cacerts in istio-system namespace (local cluster cacerts)
    #   - LOCAL_CLUSTER-cacerts include cacerts data along with remote-secret

    export PATH=$PATH:/opt/bitnami/kubectl/bin:/usr/local/bin
    which kubectl || echo "WARNING: Kubectl not found"

    NAMESPACE="${NAMESPACE:?Environment variable NAMESPACE must be set}"
    KUBECONFIG_B64="${KUBECONFIG_B64:?Environment variable KUBECONFIG_B64 must be set}"
    ROOT_SECRET_NAME="${ROOT_SECRET_NAME:-istio-root-ca}"
    SCRIPTS_DIR="${SCRIPTS_DIR:-/scripts}"
    
    LOCAL_CLUSTER="k8s-skycluster-management"
    
    tmpdir="$(mktemp -d)"
    source ${SCRIPTS_DIR}/functions.sh
    echo "Load functions from ${SCRIPTS_DIR}/functions.sh"

    # Decode base64 kubeconfig
    KUBECFG_PATH=$(setup_kubeconfig "${KUBECONFIG_B64}")

    ISTIO_DIR=$(setup_istio)

    echo "---- Check/Create ROOT CA secret '${ROOT_SECRET_NAME}'…"
    if ! kubectl -n "${NAMESPACE}" get secret "${ROOT_SECRET_NAME}" >/dev/null 2>&1; then
      echo "---- Generating root CA…"
      pushd "${ISTIO_DIR}/tools" >/dev/null
      make -f ./certs/Makefile.selfsigned.mk root-ca
      popd >/dev/null

      echo "---- Creating ROOT CA secret '${ROOT_SECRET_NAME}'…"
      kubectl -n "${NAMESPACE}" create secret generic "${ROOT_SECRET_NAME}" \
        --from-file=root-cert.pem="${ISTIO_DIR}/tools/root-cert.pem" \
        --from-file=root-key.pem="${ISTIO_DIR}/tools/root-key.pem" \
        --from-file=root-ca.conf="${ISTIO_DIR}/tools/root-ca.conf" \
        --from-file=root-cert.csr="${ISTIO_DIR}/tools/root-cert.csr" \
        --dry-run=client -o yaml \
      | kubectl label --local -f - --overwrite \
          "skycluster.io/managed-by=skycluster" \
          "skycluster.io/secret-type=istio-root-ca" -o yaml \
      | kubectl -n "${NAMESPACE}" apply -f -
      ROOT_ACTION="created"
    else
      ROOT_ACTION="reused"
      echo "---- Root CA secret exists; reuse."
    fi

    # Creating istio ca certs for the local cluster just in case
    # later we initialize a multi-cluster setup
    # the local ca certs for the local cluster should named "cacerts"

    SECRET_NAME="cacerts"

    echo "---- Creating local cacerts secret '${SECRET_NAME}'…"
    if ! kubectl -n istio-system get secret "${SECRET_NAME}" >/dev/null 2>&1; then
      echo "---- Generating intermediate CA for local cluster"
      pushd "${ISTIO_DIR}/tools" >/dev/null
      make -f ./certs/Makefile.selfsigned.mk "${LOCAL_CLUSTER}-cacerts"
      popd >/dev/null

      generated_root="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/root-cert.pem"
      expected_root="${ISTIO_DIR}/tools/root-cert.pem"
      if ! cmp -s "${generated_root}" "${expected_root}"; then
        echo "Error: Generated root-cert.pem does not match ${expected_root}" >&2
        exit 1
      fi

      if ! kubectl get ns istio-system >/dev/null 2>&1; then
        echo "Creating namespace istio-system..."
        kubectl create namespace istio-system
      fi

      kubectl -n istio-system create secret generic "${SECRET_NAME}" \
        --from-file=ca-cert.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/ca-cert.pem" \
        --from-file=ca-key.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/ca-key.pem" \
        --from-file=cert-chain.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/cert-chain.pem" \
        --from-file=root-cert.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/root-cert.pem" \
        --dry-run=client -o yaml \
      | kubectl label --local -f - --overwrite \
          "skycluster.io/managed-by=skycluster" \
          "skycluster.io/cluster-name=${LOCAL_CLUSTER}" \
          --dry-run=client -o yaml \
      | kubectl -n istio-system apply -f -
      created_cacerts+=("${LOCAL_CLUSTER}")
    else
      echo "---- Found local cacerts secret; reuse."
      reused_cacerts+=("${LOCAL_CLUSTER}")
    fi

    # combine the cluster cacerts with remote-secret data just to be consistent
    # with the way we setup remote clusters
    echo "---- Creating remote-secret for ${LOCAL_CLUSTER}…"
    if ! kubectl -n "${NAMESPACE}" get secret "${LOCAL_CLUSTER}-cacerts" >/dev/null 2>&1; then
      echo "---- Storing generated cacerts for intermediate CA for local cluster: ${LOCAL_CLUSTER}-cacerts"

      istioctl create-remote-secret --kubeconfig="${KUBECFG_PATH}" \
        --name="${LOCAL_CLUSTER}" > "${tmpdir}/remote-secret.yaml"
      
      # create and with remote-secret.yaml
      kubectl -n "${NAMESPACE}" create secret generic "${LOCAL_CLUSTER}-cacerts" \
        --from-file=ca-cert.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/ca-cert.pem" \
        --from-file=ca-key.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/ca-key.pem" \
        --from-file=cert-chain.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/cert-chain.pem" \
        --from-file=root-cert.pem="${ISTIO_DIR}/tools/${LOCAL_CLUSTER}/root-cert.pem" \
        --from-file=remote-secret.yaml="${tmpdir}/remote-secret.yaml" \
        --dry-run=client -o yaml \
      | kubectl label --local -f - --overwrite \
          "skycluster.io/managed-by=skycluster" \
          "skycluster.io/secret-type=cluster-cacert" \
          "skycluster.io/cluster-name=${LOCAL_CLUSTER}" \
          --dry-run=client -o yaml \
      | kubectl -n "${NAMESPACE}" apply -f -
      created_cacerts+=("${LOCAL_CLUSTER}")
    else
      echo "---- Found local cacerts secret; reuse."
      reused_cacerts+=("${LOCAL_CLUSTER}")
    fi
  
    echo "----- RESULT SUMMARY -----"
    echo "root_ca: ${ROOT_ACTION}"
    
    echo "SUCCESS"
    exit 0

  istio-cluster-cacerts.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    # This script creates local CA for the given cluster names,
    # it fetch connection-data using label skycluster.io/secret-type=k8s-connection-data
    # and use make file to generate CAs and istioctl to create the necessary certificates
    # for remote clusters and store all data in <cluster-name>-cacerts secret.
    # If the secret exists, it will be reused.

    export PATH=$PATH:/opt/bitnami/kubectl/bin:/usr/local/bin
    which kubectl || echo "WARNING: Kubectl not found"


    NAMESPACE="${NAMESPACE:?Environment variable NAMESPACE must be set}"
    ROOT_SECRET_NAME="${ROOT_SECRET_NAME:-istio-root-ca}"
    KCFG_SELECTOR="${KCFG_SELECTOR:-skycluster.io/secret-type=k8s-connection-data}"
    OWNER="${OWNER:-}"
    OWNER_PATCH="${OWNER_PATCH:-}"
    SCRIPTS_DIR="${SCRIPTS_DIR:-/scripts}"

    source ${SCRIPTS_DIR}/functions.sh

    ISTIO_DIR=$(setup_istio)
    echo "---- Istio tools dir: ${ISTIO_DIR}"

    echo "---- Check ROOT CA secret '${ROOT_SECRET_NAME}'…"
    if ! kubectl -n "${NAMESPACE}" get secret "${ROOT_SECRET_NAME}" >/dev/null 2>&1; then
      echo "---- ERROR: ROOT CA does not exist"
      exit 1
    else
      echo "---- Root CA secret exists."
      _pull_secret_key "root-cert.pem" "${ISTIO_DIR}/tools/root-cert.pem"
      _pull_secret_key "root-key.pem"  "${ISTIO_DIR}/tools/root-key.pem"
      _pull_secret_key "root-ca.conf"  "${ISTIO_DIR}/tools/root-ca.conf"
      _pull_secret_key "root-cert.csr" "${ISTIO_DIR}/tools/root-cert.csr"

      # make them newer so make does not recreate them
      chmod 600 "${ISTIO_DIR}/tools/root-key.pem"
      touch "${ISTIO_DIR}/tools/root-cert.pem"
    fi    

    echo "---- List kubeconfig secrets clients (selector: ${KCFG_SELECTOR})"
    all_kcfg_json="$(kubectl -n "${NAMESPACE}" get secret -l "${KCFG_SELECTOR}" -o json)"

    # Parse cluster list, we generate cacerts for all secrets available, 
    # TODO: make sure the list is filtered based on user input
    mapfile -t CLUSTER_LIST < <(
      jq -r '.items[] | "\(.metadata.name)\t\(.metadata.labels["skycluster.io/cluster-name"])\t\(.metadata.labels["skycluster.io/provider-platform"] // "")"' <<< "${all_kcfg_json}" \
        | sed '/^\s*$/d' \
        | sort -u
    )

    created_cacerts=()
    reused_cacerts=()
    patched_remote=()

    get_domain() {
      case "$1" in
        gcp) echo "gcp.skycluster.io" ;;
        aws) echo "aws.skycluster.io" ;;
        openstack) echo "os.skycluster.io" ;;
        *) echo "Unknown provider" ;;
      esac
    }

    tmpdir="$(mktemp -d)"
    KUBECFG_PATH="${tmpdir}/kubeconfig.yaml"

    for ENTRY in "${CLUSTER_LIST[@]:-}"; do
      C=$(awk -F'\t' '{print $1}' <<< "$ENTRY")
      CLUSTER_NAME=$(awk -F'\t' '{print $2}' <<< "$ENTRY")
      PLATFORM=$(awk -F'\t' '{print $3}' <<< "$ENTRY")
      echo "---- Cluster: ${C} and cluster name: ${CLUSTER_NAME} and provider platform: ${PLATFORM}"

      # Extract kubeconfig to temp
      tmpdir="$(mktemp -d)"
      secretData=$(kubectl -n "${NAMESPACE}" get secret "${C}" -o json)
      echo "$secretData" | jq -r '.data.kubeconfig' | base64 -d > "${KUBECFG_PATH}"

      # if platform is gcp export GOOGLE_APPLICATION_CREDENTIALS
      if [[ "${PLATFORM}" == "gcp" ]]; then
        gcp_key_b64=$(echo "$secretData" | jq -r '.data["privateKey"] // empty')
        if [[ -n "$gcp_key_b64" ]]; then
          gcp_key_path="${tmpdir}/gcp-service-account.json"
          echo "$gcp_key_b64" | base64 -d > "$gcp_key_path"
          export GOOGLE_APPLICATION_CREDENTIALS="$gcp_key_path"
          echo "---- Set GOOGLE_APPLICATION_CREDENTIALS for GCP cluster ${C}"
        else
          echo "---- WARNING: GCP cluster ${C} missing gcp-service-account.json in secret; skipping setting GOOGLE_APPLICATION_CREDENTIALS"
        fi
      fi

      CACERTS_SECRET="${C}-cacerts"
      C_DNSSAFE="${CLUSTER_NAME//./-}"
      echo "---- DNS Safe Label: ${C_DNSSAFE}"
      if ! kubectl -n "${NAMESPACE}" get secret "${CACERTS_SECRET}" >/dev/null 2>&1; then
        echo "---- Generating intermediate CA for ${C}…"
        pushd "${ISTIO_DIR}/tools" >/dev/null
        make -f ./certs/Makefile.selfsigned.mk "${C}-cacerts"
        popd >/dev/null
        
        API_VERSION=$(get_domain "${PLATFORM}") 
        OBJ_UID=$(kubectl get "xkubes.${API_VERSION}" "${CLUSTER_NAME}" -o jsonpath='{.metadata.uid}')
        
        kubectl -n "${NAMESPACE}" create secret generic "${CACERTS_SECRET}" \
          --from-file=ca-cert.pem="${ISTIO_DIR}/tools/${C}/ca-cert.pem" \
          --from-file=ca-key.pem="${ISTIO_DIR}/tools/${C}/ca-key.pem" \
          --from-file=cert-chain.pem="${ISTIO_DIR}/tools/${C}/cert-chain.pem" \
          --from-file=root-cert.pem="${ISTIO_DIR}/tools/${C}/root-cert.pem" \
          --dry-run=client -o yaml \
        | kubectl label --local -f - --overwrite \
            "skycluster.io/managed-by=skycluster" \
            "skycluster.io/secret-type=cluster-cacert" \
            "skycluster.io/cluster-name=${C_DNSSAFE}" \
            --dry-run=client -o yaml \
        | kubectl patch --local -f - -p "$(cat <<EOF
    metadata:
      ownerReferences:
      - apiVersion: $API_VERSION/v1alpha1
        kind: XKube
        name: ${CLUSTER_NAME}
        uid: ${OBJ_UID}
        controller: true
        blockOwnerDeletion: true
    EOF
        )" --type=merge -o yaml \
        | kubectl -n "${NAMESPACE}" apply -f -

        created_cacerts+=("$C")
      else
        echo "---- Found ${CACERTS_SECRET}; reuse."
        reused_cacerts+=("$C")
      fi

      _helper_set_status "${NAMESPACE}" "cacerts-status" "${CLUSTER_NAME}" "true" "$OWNER" "$OWNER_PATCH"

      patched_remote+=("$C")
    done


    echo "----- RESULT SUMMARY -----"
    echo "created_cacerts: ${created_cacerts[*]-}"
    echo "reused_cacerts: ${reused_cacerts[*]-}"
    echo "patched_remote_secrets: ${patched_remote[*]-}"
    
    echo "SUCCESS"
    exit 0

  cleanup.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    export PATH=$PATH:/opt/bitnami/kubectl/bin:/usr/local/bin
    command -v kubectl >/dev/null || echo "WARNING: Kubectl not found"
    command -v helm >/dev/null || echo "WARNING: helm not found in PATH"

    : "${CHART_NAMESPACE:?CHART_NAMESPACE must be set}"
    : "${KUBECONFIG_B64:?KUBECONFIG_B64 must be set (base64 kubeconfig or 'local')}"

    CHART_NAME="${CHART_NAME:-}"
    CLUSTER_NAME="${CLUSTER_NAME:?Cluster name must be set}"
    BLOCKING_OBJECTS="${BLOCKING_OBJECTS:-}"                 # e.g., "serviceaccount/foo configmap/bar"
    CLUSTERROLE_PREFIX="${CLUSTERROLE_PREFIX:-}"             # eg. "submariner"
    OWNER="${OWNER:-}"
    OWNER_PATCH="${OWNER_PATCH:-}"
    SCRIPTS_DIR="${SCRIPTS_DIR:-/scripts}"

    source ${SCRIPTS_DIR}/functions.sh

    KUBECONFIG_ARG=()
    if [[ "$KUBECONFIG_B64" != "local" ]]; then
      # Decode base64 kubeconfig
      KUBECFG_PATH=$(setup_kubeconfig "${KUBECONFIG_B64}")
      KUBECONFIG_ARG=(--kubeconfig "$KUBECFG_PATH")
      printf "Cluster API server: "
      kubectl "${KUBECONFIG_ARG[@]}" config view --minify -o jsonpath='{.clusters[0].cluster.server}'; echo
    else
      echo "Using local kubeconfig"
      echo "https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
    fi

    # --- Namespaced cleanup ---
    if ! kubectl "${KUBECONFIG_ARG[@]}" get ns "$CHART_NAMESPACE" >/dev/null 2>&1; then
      echo "Namespace '$CHART_NAMESPACE' not found; skipping namespaced cleanup."
    else
      releases="$(helm "${KUBECONFIG_ARG[@]}" list -n "$CHART_NAMESPACE" -q 2>/dev/null || true)"
      if [[ -z "$releases" ]]; then
        echo "No Helm releases found in namespace $CHART_NAMESPACE."
      else
        for r in $releases; do
          echo "Uninstalling Helm release: $r"
          helm "${KUBECONFIG_ARG[@]}" uninstall "$r" -n "$CHART_NAMESPACE" || true
        done
      fi

      echo "Deleting explicitly-blocking objects (with finalizer patch if needed)…"
      for obj in $BLOCKING_OBJECTS; do
        kind="${obj%%/*}"; name="${obj##*/}"
        if kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" get "$kind" "$name" >/dev/null 2>&1; then
          # Try normal delete first
          kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" delete "$kind" "$name" --ignore-not-found --wait=false || true

          # If it lingers, remove finalizers via patch, then delete again
          if kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" get "$kind" "$name" >/dev/null 2>&1; then
            echo "Patching finalizers off $kind/$name …"
            kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" patch "$kind" "$name" --type=merge -p '{"metadata":{"finalizers":[]}}' || true
            kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" delete "$kind" "$name" --ignore-not-found --wait=false || true
          fi

          # Last resort
          kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" get "$kind" "$name" >/dev/null 2>&1 \
            && kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" delete "$kind" "$name" --force --grace-period=0 || true
        else
          echo "$kind/$name not present in $CHART_NAMESPACE."
        fi
      done

      echo "Deleting ServiceAccounts starting with prefix '$CLUSTERROLE_PREFIX' in namespace $CHART_NAMESPACE..."
      for round in {1..2}; do
        sa_list="$(kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" get sa -o name 2>/dev/null | grep "^serviceaccount/${CLUSTERROLE_PREFIX}" || true)"
        if [[ -n "$sa_list" ]]; then
          for sa in $sa_list; do
            echo "Deleting $sa"
            kubectl "${KUBECONFIG_ARG[@]}" -n "$CHART_NAMESPACE" delete "$sa" --ignore-not-found --wait=false || true
          done
          break
        else
          echo "No ServiceAccounts with prefix '$CLUSTERROLE_PREFIX' found in $CHART_NAMESPACE. Try again..."
        fi
        sleep 1
      done

    fi


    # --- Cluster-scoped cleanup: ClusterRoles by prefix ---
    if [[ -n "$CLUSTERROLE_PREFIX" ]]; then
      echo "=== ClusterRole cleanup for prefix: '$CLUSTERROLE_PREFIX' ==="
      cr_list="$(kubectl "${KUBECONFIG_ARG[@]}" get clusterrole -o name 2>/dev/null \
        | grep -E "^clusterrole(\.rbac\.authorization\.k8s\.io)?/${CLUSTERROLE_PREFIX}" || true)"
      if [[ -n "$cr_list" ]]; then
        for cr in $cr_list; do
          echo "Deleting $cr"
          kubectl "${KUBECONFIG_ARG[@]}" delete "$cr" --ignore-not-found --wait=false || true
        done
      else
        echo "No ClusterRoles with prefix '$CLUSTERROLE_PREFIX' found."
      fi
    fi


    # --- Cluster-scoped cleanup: ClusterRoleBindings by prefix ---
    if [[ -n "$CLUSTERROLE_PREFIX" ]]; then
      echo "=== ClusterRoleBinding cleanup for prefix: '$CLUSTERROLE_PREFIX' ==="
      # List ClusterRoleBindings whose names start with the prefix
      mapfile -t crbs_to_delete < <(
        kubectl "${KUBECONFIG_ARG[@]}" get clusterrolebinding -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' \
          | grep -E "^${CLUSTERROLE_PREFIX}" || true
      )

      if ((${#crbs_to_delete[@]} == 0)); then
        echo "No ClusterRoleBindings found with prefix '${CLUSTERROLE_PREFIX}'."
      else
        for crb in "${crbs_to_delete[@]}"; do
          echo "Deleting ClusterRoleBinding: $crb"
          # Try normal delete first
          kubectl "${KUBECONFIG_ARG[@]}" delete clusterrolebinding "$crb" --ignore-not-found --wait=false || true

          # If it lingers (rare for CRBs), clear finalizers then delete again
          if kubectl "${KUBECONFIG_ARG[@]}" get clusterrolebinding "$crb" >/dev/null 2>&1; then
            echo "Patching finalizers off clusterrolebinding/$crb …"
            kubectl "${KUBECONFIG_ARG[@]}" patch clusterrolebinding "$crb" --type=merge -p '{"metadata":{"finalizers":[]}}' || true
            kubectl "${KUBECONFIG_ARG[@]}" delete clusterrolebinding "$crb" --ignore-not-found --wait=false || true
          fi

          # Last resort
          kubectl "${KUBECONFIG_ARG[@]}" get clusterrolebinding "$crb" >/dev/null 2>&1 \
            && kubectl "${KUBECONFIG_ARG[@]}" delete clusterrolebinding "$crb" --force --grace-period=0 || true
        done
      fi
    fi

    _helper_set_status "skycluster-system" "cleanup-${CHART_NAME}" "${CLUSTER_NAME}" "true" "$OWNER" "$OWNER_PATCH"
    echo "Cleanup complete for namespace $CHART_NAMESPACE."
 
  istio-remote-secret.sh: |
    #!/usr/bin/env bash
    set -euo pipefail

    # This script uses istioctl to create the necessary certificates
    # for remote clusters and store all data in <cluster-name>-cacerts secret.
    # If the secret exists, it will be reused.

    export PATH=$PATH:/opt/bitnami/kubectl/bin:/usr/local/bin
    which kubectl || echo "WARNING: Kubectl not found"

    NAMESPACE="${NAMESPACE:?Environment variable NAMESPACE must be set}"
    OWNER="${OWNER:-}"
    OWNER_PATCH="${OWNER_PATCH:-}"
    SCRIPTS_DIR="${SCRIPTS_DIR:-/scripts}"
    CLUSTER_NAME=${CLUSTER_NAME:?"Environment variable CLUSTER_NAME must be set"}

    source ${SCRIPTS_DIR}/functions.sh

    ISTIO_DIR=$(setup_istio)
    echo "---- Istio tools dir: ${ISTIO_DIR}"

    tmpdir="$(mktemp -d)"
    KUBECFG_PATH="${tmpdir}/kubeconfig.yaml"

    echo "---- List kubeconfig secrets clients"
    all_kcfg_json="$(kubectl -n "${NAMESPACE}" get secret -l "skycluster.io/secret-type=k8s-connection-data,skycluster.io/cluster-name=${CLUSTER_NAME}" -o json)"

    # Parse cluster list, we generate cacerts for all secrets available, 
    # TODO: make sure the list is filtered based on user input
    mapfile -t CLUSTER_LIST < <(
      jq -r '.items[] | "\(.metadata.name)\t\(.metadata.labels["skycluster.io/cluster-name"])\t\(.metadata.labels["skycluster.io/provider-platform"] // "")"' <<< "${all_kcfg_json}" \
        | sed '/^\s*$/d' \
        | sort -u
    )

    for ENTRY in "${CLUSTER_LIST[@]:-}"; do
      C=$(awk -F'\t' '{print $1}' <<< "$ENTRY")
      PLATFORM=$(awk -F'\t' '{print $3}' <<< "$ENTRY")
      echo "---- Cluster: ${C} and cluster name: ${CLUSTER_NAME} and provider platform: ${PLATFORM}"
   
      # Extract kubeconfig to temp
      secretData=$(kubectl -n "${NAMESPACE}" get secret "${C}" -o json)
      echo "$secretData" | jq -r '.data.kubeconfig' | base64 -d > "${KUBECFG_PATH}"

      # if platform is gcp export GOOGLE_APPLICATION_CREDENTIALS
      if [[ "${PLATFORM}" == "gcp" ]]; then
        gcp_key_b64=$(echo "$secretData" | jq -r '.data["privateKey"] // empty')
        if [[ -n "$gcp_key_b64" ]]; then
          gcp_key_path="${tmpdir}/gcp-service-account.json"
          echo "$gcp_key_b64" | base64 -d > "$gcp_key_path"
          export GOOGLE_APPLICATION_CREDENTIALS="$gcp_key_path"
          echo "---- Set GOOGLE_APPLICATION_CREDENTIALS for GCP cluster ${C}"

          gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
          ACCESS_TOKEN=$(gcloud auth print-access-token)
          USER=$(kubectl config view --kubeconfig $KUBECFG_PATH -o jsonpath='{.users[0].name}')
          kubectl config set-credentials "$USER" \
            --token="$ACCESS_TOKEN" \
            --kubeconfig $KUBECFG_PATH
        else
          echo "---- WARNING: GCP cluster ${C} missing gcp-service-account.json in secret; skipping setting GOOGLE_APPLICATION_CREDENTIALS"
        fi
      fi
    
      CACERTS_SECRET="${C}-cacerts"
      C_DNSSAFE="${CLUSTER_NAME//./-}"
      echo "---- DNS Safe Label: ${C_DNSSAFE}"
      if ! kubectl -n "${NAMESPACE}" get secret "${CACERTS_SECRET}" >/dev/null 2>&1; then
        # Secret does not exist, it was expected to be created by cacerts pod
        echo "Missing cacert secret: ${CACERTS_SECRET}"
        exit 1
      fi

      echo "---- Creating and patching remote-secret for ${C}, kubeconfig: ${KUBECFG_PATH}"
      istioctl create-remote-secret --kubeconfig="${KUBECFG_PATH}" --name="${C}" > "${tmpdir}/remote-secret.yaml"
      remote_content="$(cat "${tmpdir}/remote-secret.yaml")"
      b64_remote="$(base64 -w0 < "${tmpdir}/remote-secret.yaml")"
      echo "---- Generated remote secret ${tmpdir}/remote-secret.yaml"

      # Patch kubeconfig secret with remote-secret.yaml (data field)
      echo "---- Patching kubeconfig secret ${CACERTS_SECRET} with remote-secret.yaml"
      kubectl -n "${NAMESPACE}" get secret "${CACERTS_SECRET}" -o json \
        | jq --arg v "${b64_remote}" '.data["remote-secret.yaml"]=$v' \
        | kubectl label --local -f - --overwrite \
          "skycluster.io/managed-by=skycluster" \
          "skycluster.io/cluster-name=${C_DNSSAFE}" \
          --dry-run=client -o yaml \
        | kubectl -n "${NAMESPACE}" apply -f -
              
      echo "---- Patched remote-secret for ${C}."
      _helper_set_status "${NAMESPACE}" "remote-secret-status" "${CLUSTER_NAME}" "true" "$OWNER" "$OWNER_PATCH"

      # Expect only one secret with cluster-name
    done


    echo "SUCCESS: ${patched_remote[*]-}"
    exit 0

  aws-src-dst-check.sh: |
    #!/bin/sh
    set -e

    NODE_NAME=${NODE_NAME:?Environment variable NODE_NAME must be set}
    AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:?Environment variable AWS_DEFAULT_REGION must be set}

    echo "Ensuring source/destination check disabled on node $NODE_NAME"
    
    INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
    echo "Instance ID: $INSTANCE_ID"

    ENI_IDS=$(aws ec2 describe-instances \
      --instance-ids $INSTANCE_ID \
      --query "Reservations[].Instances[].NetworkInterfaces[].NetworkInterfaceId" \
      --output text)

    for eni in $ENI_IDS; do
      echo "Disabling source-dest-check on ENI: $eni"
      aws ec2 modify-network-interface-attribute \
        --network-interface-id $eni \
        --no-source-dest-check || true
    done

    echo "Done. Sleeping to stay alive..."
    sleep 3600
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: provider-metadata-e2e
  namespace: skycluster-system
  labels:
    skycluster.io/config-type: provider-metadata-e2e
data:
  gcp: |
    - vpcCidr: 10.16.0.0/16
      subnets:
        - cidr: 10.16.0.0/18
    - vpcCidr: 10.17.0.0/16
      subnets:
        - cidr: 10.17.0.0/18
    - vpcCidr: 10.18.0.0/16
      subnets:
        - cidr: 10.18.0.0/18
    - vpcCidr: 10.19.0.0/16
      subnets:
        - cidr: 10.19.0.0/18
    - vpcCidr: 10.20.0.0/16
      subnets:
        - cidr: 10.20.0.0/18
    - vpcCidr: 10.21.0.0/16
      subnets:
        - cidr: 10.21.0.0/18
    - vpcCidr: 10.22.0.0/16
      subnets:
        - cidr: 10.22.0.0/18
    - vpcCidr: 10.23.0.0/16
      subnets:
        - cidr: 10.23.0.0/18
    - vpcCidr: 10.24.0.0/16
      subnets:
        - cidr: 10.24.0.0/18
    - vpcCidr: 10.25.0.0/16
      subnets:
        - cidr: 10.25.0.0/18
    - vpcCidr: 10.26.0.0/16
      subnets:
        - cidr: 10.26.0.0/18
    - vpcCidr: 10.27.0.0/16
      subnets:
        - cidr: 10.27.0.0/18
    - vpcCidr: 10.28.0.0/16
      subnets:
        - cidr: 10.28.0.0/18
    - vpcCidr: 10.29.0.0/16
      subnets:
        - cidr: 10.29.0.0/18
    - vpcCidr: 10.30.0.0/16
      subnets:
        - cidr: 10.30.0.0/18
    - vpcCidr: 10.31.0.0/16
      subnets:
        - cidr: 10.31.0.0/18
    - vpcCidr: 10.32.0.0/16
      subnets:
        - cidr: 10.32.0.0/18
  aws: |
    - vpcCidr: 10.33.0.0/16
      subnets:
        - type: public
          cidr: 10.33.0.0/19
        - type: private
          cidr: 10.33.32.0/19
    - vpcCidr: 10.34.0.0/16
      subnets:
        - type: public
          cidr: 10.34.0.0/19
        - type: private
          cidr: 10.34.32.0/19
    - vpcCidr: 10.35.0.0/16
      subnets:
        - type: public
          cidr: 10.35.0.0/19
        - type: private
          cidr: 10.35.32.0/19
    - vpcCidr: 10.36.0.0/16
      subnets:
        - type: public
          cidr: 10.36.0.0/19
        - type: private
          cidr: 10.36.32.0/19
    - vpcCidr: 10.37.0.0/16
      subnets:
        - type: public
          cidr: 10.37.0.0/19
        - type: private
          cidr: 10.37.32.0/19
    - vpcCidr: 10.38.0.0/16
      subnets:
        - type: public
          cidr: 10.38.0.0/19
        - type: private
          cidr: 10.38.32.0/19
    - vpcCidr: 10.39.0.0/16
      subnets:
        - type: public
          cidr: 10.39.0.0/19
        - type: private
          cidr: 10.39.32.0/19
    - vpcCidr: 10.40.0.0/16
      subnets:
        - type: public
          cidr: 10.40.0.0/19
        - type: private
          cidr: 10.40.32.0/19
    - vpcCidr: 10.41.0.0/16
      subnets:
        - type: public
          cidr: 10.41.0.0/19
        - type: private
          cidr: 10.41.32.0/19
    - vpcCidr: 10.42.0.0/16
      subnets:
        - type: public
          cidr: 10.42.0.0/19
        - type: private
          cidr: 10.42.32.0/19
    - vpcCidr: 10.43.0.0/16
      subnets:
        - type: public
          cidr: 10.43.0.0/19
        - type: private
          cidr: 10.43.32.0/19
    - vpcCidr: 10.44.0.0/16
      subnets:
        - type: public
          cidr: 10.44.0.0/19
        - type: private
          cidr: 10.44.32.0/19
    - vpcCidr: 10.45.0.0/16
      subnets:
        - type: public
          cidr: 10.45.0.0/19
        - type: private
          cidr: 10.45.32.0/19
    - vpcCidr: 10.46.0.0/16
      subnets:
        - type: public
          cidr: 10.46.0.0/19
        - type: private
          cidr: 10.46.32.0/19
    - vpcCidr: 10.47.0.0/16
      subnets:
        - type: public
          cidr: 10.47.0.0/19
        - type: private
          cidr: 10.47.32.0/19
    - vpcCidr: 10.48.0.0/16
      subnets:
        - type: public
          cidr: 10.48.0.0/19
        - type: private
          cidr: 10.48.32.0/19
    - vpcCidr: 10.49.0.0/16
      subnets:
        - type: public
          cidr: 10.49.0.0/19
        - type: private
          cidr: 10.49.32.0/19
    - vpcCidr: 10.50.0.0/16
      subnets:
        - type: public
          cidr: 10.50.0.0/19
        - type: private
          cidr: 10.50.32.0/19
    - vpcCidr: 10.51.0.0/16
      subnets:
        - type: public
          cidr: 10.51.0.0/19
        - type: private
          cidr: 10.51.32.0/19
    - vpcCidr: 10.52.0.0/16
      subnets:
        - type: public
          cidr: 10.52.0.0/19
        - type: private
          cidr: 10.52.32.0/19
    - vpcCidr: 10.53.0.0/16
      subnets:
        - type: public
          cidr: 10.53.0.0/19
        - type: private
          cidr: 10.53.32.0/19
    - vpcCidr: 10.54.0.0/16
      subnets:
        - type: public
          cidr: 10.54.0.0/19
        - type: private
          cidr: 10.54.32.0/19
    - vpcCidr: 10.55.0.0/16
      subnets:
        - type: public
          cidr: 10.55.0.0/19
        - type: private
          cidr: 10.55.32.0/19
    - vpcCidr: 10.56.0.0/16
      subnets:
        - type: public
          cidr: 10.56.0.0/19
        - type: private
          cidr: 10.56.32.0/19
    - vpcCidr: 10.57.0.0/16
      subnets:
        - type: public
          cidr: 10.57.0.0/19
        - type: private
          cidr: 10.57.32.0/19
    - vpcCidr: 10.58.0.0/16
      subnets:
        - type: public
          cidr: 10.58.0.0/19
        - type: private
          cidr: 10.58.32.0/19
    - vpcCidr: 10.59.0.0/16
      subnets:
        - type: public
          cidr: 10.59.0.0/19
        - type: private
          cidr: 10.59.32.0/19
    - vpcCidr: 10.60.0.0/16
      subnets:
        - type: public
          cidr: 10.60.0.0/19
        - type: private
          cidr: 10.60.32.0/19
    - vpcCidr: 10.61.0.0/16
      subnets:
        - type: public
          cidr: 10.61.0.0/19
        - type: private
          cidr: 10.61.32.0/19
    - vpcCidr: 10.62.0.0/16
      subnets:
        - type: public
          cidr: 10.62.0.0/19
        - type: private
          cidr: 10.62.32.0/19
    - vpcCidr: 10.63.0.0/16
      subnets:
        - type: public
          cidr: 10.63.0.0/19
        - type: private
          cidr: 10.63.32.0/19
    - vpcCidr: 10.64.0.0/16
      subnets:
        - type: public
          cidr: 10.64.0.0/19
        - type: private
          cidr: 10.64.32.0/19
    - vpcCidr: 10.65.0.0/16
      subnets:
        - type: public
          cidr: 10.65.0.0/19
        - type: private
          cidr: 10.65.32.0/19
    - vpcCidr: 10.66.0.0/16
      subnets:
        - type: public
          cidr: 10.66.0.0/19
        - type: private
          cidr: 10.66.32.0/19
    - vpcCidr: 10.67.0.0/16
      subnets:
        - type: public
          cidr: 10.67.0.0/19
        - type: private
          cidr: 10.67.32.0/19
    - vpcCidr: 10.68.0.0/16
      subnets:
        - type: public
          cidr: 10.68.0.0/19
        - type: private
          cidr: 10.68.32.0/19
    - vpcCidr: 10.69.0.0/16
      subnets:
        - type: public
          cidr: 10.69.0.0/19
        - type: private
          cidr: 10.69.32.0/19
    - vpcCidr: 10.70.0.0/16
      subnets:
        - type: public
          cidr: 10.70.0.0/19
        - type: private
          cidr: 10.70.32.0/19
    - vpcCidr: 10.71.0.0/16
      subnets:
        - type: public
          cidr: 10.71.0.0/19
        - type: private
          cidr: 10.71.32.0/19
    - vpcCidr: 10.72.0.0/16
      subnets:
        - type: public
          cidr: 10.72.0.0/19
        - type: private
          cidr: 10.72.32.0/19
    - vpcCidr: 10.73.0.0/16
      subnets:
        - type: public
          cidr: 10.73.0.0/19
        - type: private
          cidr: 10.73.32.0/19
    - vpcCidr: 10.74.0.0/16
      subnets:
        - type: public
          cidr: 10.74.0.0/19
        - type: private
          cidr: 10.74.32.0/19
    - vpcCidr: 10.75.0.0/16
      subnets:
        - type: public
          cidr: 10.75.0.0/19
        - type: private
          cidr: 10.75.32.0/19
    - vpcCidr: 10.76.0.0/16
      subnets:
        - type: public
          cidr: 10.76.0.0/19
        - type: private
          cidr: 10.76.32.0/19
    - vpcCidr: 10.77.0.0/16
      subnets:
        - type: public
          cidr: 10.77.0.0/19
        - type: private
          cidr: 10.77.32.0/19
    - vpcCidr: 10.78.0.0/16
      subnets:
        - type: public
          cidr: 10.78.0.0/19
        - type: private
          cidr: 10.78.32.0/19
    - vpcCidr: 10.79.0.0/16
      subnets:
        - type: public
          cidr: 10.79.0.0/19
        - type: private
          cidr: 10.79.32.0/19
    - vpcCidr: 10.80.0.0/16
      subnets:
        - type: public
          cidr: 10.80.0.0/19
        - type: private
          cidr: 10.80.32.0/19
    - vpcCidr: 10.81.0.0/16
      subnets:
        - type: public
          cidr: 10.81.0.0/19
        - type: private
          cidr: 10.81.32.0/19
    - vpcCidr: 10.82.0.0/16
      subnets:
        - type: public
          cidr: 10.82.0.0/19
        - type: private
          cidr: 10.82.32.0/19
    - vpcCidr: 10.83.0.0/16
      subnets:
        - type: public
          cidr: 10.83.0.0/19
        - type: private
          cidr: 10.83.32.0/19
    - vpcCidr: 10.84.0.0/16
      subnets:
        - type: public
          cidr: 10.84.0.0/19
        - type: private
          cidr: 10.84.32.0/19
    - vpcCidr: 10.85.0.0/16
      subnets:
        - type: public
          cidr: 10.85.0.0/19
        - type: private
          cidr: 10.85.32.0/19
    - vpcCidr: 10.86.0.0/16
      subnets:
        - type: public
          cidr: 10.86.0.0/19
        - type: private
          cidr: 10.86.32.0/19
    - vpcCidr: 10.87.0.0/16
      subnets:
        - type: public
          cidr: 10.87.0.0/19
        - type: private
          cidr: 10.87.32.0/19
    - vpcCidr: 10.88.0.0/16
      subnets:
        - type: public
          cidr: 10.88.0.0/19
        - type: private
          cidr: 10.88.32.0/19
    - vpcCidr: 10.89.0.0/16
      subnets:
        - type: public
          cidr: 10.89.0.0/19
        - type: private
          cidr: 10.89.32.0/19
    - vpcCidr: 10.90.0.0/16
      subnets:
        - type: public
          cidr: 10.90.0.0/19
        - type: private
          cidr: 10.90.32.0/19
    - vpcCidr: 10.91.0.0/16
      subnets:
        - type: public
          cidr: 10.91.0.0/19
        - type: private
          cidr: 10.91.32.0/19
    - vpcCidr: 10.92.0.0/16
      subnets:
        - type: public
          cidr: 10.92.0.0/19
        - type: private
          cidr: 10.92.32.0/19
    - vpcCidr: 10.93.0.0/16
      subnets:
        - type: public
          cidr: 10.93.0.0/19
        - type: private
          cidr: 10.93.32.0/19
    - vpcCidr: 10.94.0.0/16
      subnets:
        - type: public
          cidr: 10.94.0.0/19
        - type: private
          cidr: 10.94.32.0/19
    - vpcCidr: 10.95.0.0/16
      subnets:
        - type: public
          cidr: 10.95.0.0/19
        - type: private
          cidr: 10.95.32.0/19
  openstack: |
    - vpcCidr: 10.82.0.0/17
      subnets:
        - cidr: 10.82.0.0/18
    - vpcCidr: 10.83.0.0/17
      subnets:
        - cidr: 10.83.0.0/18
    - vpcCidr: 10.84.0.0/17
      subnets:
        - cidr: 10.84.0.0/18
    - vpcCidr: 10.85.0.0/17
      subnets:
        - cidr: 10.85.0.0/18
    - vpcCidr: 10.86.0.0/17
      subnets:
        - cidr: 10.86.0.0/18
    - vpcCidr: 10.87.0.0/17
      subnets:
        - cidr: 10.87.0.0/18
    - vpcCidr: 10.88.0.0/17
      subnets:
        - cidr: 10.88.0.0/18
    - vpcCidr: 10.89.0.0/17
      subnets:
        - cidr: 10.89.0.0/18
    - vpcCidr: 10.90.0.0/17
      subnets:
        - cidr: 10.90.0.0/18
    - vpcCidr: 10.91.0.0/17
      subnets:
        - cidr: 10.91.0.0/18
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-cluster-metadata-e2e
  namespace: skycluster-system
  labels:
    skycluster.io/config-type: k8s-cluster-metadata-e2e
data:
  aws: |
    - serviceCidr: 10.255.0.0/16
      podCidr:
        cidr: 10.33.128.0/17
        public: 10.33.128.0/18
        private: 10.33.192.0/18
    - serviceCidr: 10.254.0.0/16
      podCidr:
        cidr: 10.34.128.0/17
        public: 10.34.128.0/18
        private: 10.34.192.0/18
    - serviceCidr: 10.253.0.0/16
      podCidr:
        cidr: 10.35.128.0/17
        public: 10.35.128.0/18
        private: 10.35.192.0/18
    - serviceCidr: 10.252.0.0/16
      podCidr:
        cidr: 10.36.128.0/17
        public: 10.36.128.0/18
        private: 10.36.192.0/18
    - serviceCidr: 10.251.0.0/16
      podCidr:
        cidr: 10.37.128.0/17
        public: 10.37.128.0/18
        private: 10.37.192.0/18
    - serviceCidr: 10.250.0.0/16
      podCidr:
        cidr: 10.38.128.0/17
        public: 10.38.128.0/18
        private: 10.38.192.0/18
    - serviceCidr: 10.249.0.0/16
      podCidr:
        cidr: 10.39.128.0/17
        public: 10.39.128.0/18
        private: 10.39.192.0/18
    - serviceCidr: 10.248.0.0/16
      podCidr:
        cidr: 10.40.128.0/17
        public: 10.40.128.0/18
        private: 10.40.192.0/18
    - serviceCidr: 10.247.0.0/16
      podCidr:
        cidr: 10.41.128.0/17
        public: 10.41.128.0/18
        private: 10.41.192.0/18
    - serviceCidr: 10.246.0.0/16
      podCidr:
        cidr: 10.42.128.0/17
        public: 10.42.128.0/18
        private: 10.42.192.0/18
    - serviceCidr: 10.245.0.0/16
      podCidr:
        cidr: 10.43.128.0/17
        public: 10.43.128.0/18
        private: 10.43.192.0/18
    - serviceCidr: 10.244.0.0/16
      podCidr:
        cidr: 10.44.128.0/17
        public: 10.44.128.0/18
        private: 10.44.192.0/18
    - serviceCidr: 10.243.0.0/16
      podCidr:
        cidr: 10.45.128.0/17
        public: 10.45.128.0/18
        private: 10.45.192.0/18
    - serviceCidr: 10.242.0.0/16
      podCidr:
        cidr: 10.46.128.0/17
        public: 10.46.128.0/18
        private: 10.46.192.0/18
    - serviceCidr: 10.241.0.0/16
      podCidr:
        cidr: 10.47.128.0/17
        public: 10.47.128.0/18
        private: 10.47.192.0/18
    - serviceCidr: 10.240.0.0/16
      podCidr:
        cidr: 10.48.128.0/17
        public: 10.48.128.0/18
        private: 10.48.192.0/18
    - serviceCidr: 10.239.0.0/16
      podCidr:
        cidr: 10.49.128.0/17
        public: 10.49.128.0/18
        private: 10.49.192.0/18
    - serviceCidr: 10.238.0.0/16
      podCidr:
        cidr: 10.50.128.0/17
        public: 10.50.128.0/18
        private: 10.50.192.0/18
    - serviceCidr: 10.237.0.0/16
      podCidr:
        cidr: 10.51.128.0/17
        public: 10.51.128.0/18
        private: 10.51.192.0/18
    - serviceCidr: 10.236.0.0/16
      podCidr:
        cidr: 10.52.128.0/17
        public: 10.52.128.0/18
        private: 10.52.192.0/18
    - serviceCidr: 10.235.0.0/16
      podCidr:
        cidr: 10.53.128.0/17
        public: 10.53.128.0/18
        private: 10.53.192.0/18
    - serviceCidr: 10.234.0.0/16
      podCidr:
        cidr: 10.54.128.0/17
        public: 10.54.128.0/18
        private: 10.54.192.0/18
    - serviceCidr: 10.233.0.0/16
      podCidr:
        cidr: 10.55.128.0/17
        public: 10.55.128.0/18
        private: 10.55.192.0/18
    - serviceCidr: 10.232.0.0/16
      podCidr:
        cidr: 10.56.128.0/17
        public: 10.56.128.0/18
        private: 10.56.192.0/18
    - serviceCidr: 10.231.0.0/16
      podCidr:
        cidr: 10.57.128.0/17
        public: 10.57.128.0/18
        private: 10.57.192.0/18
    - serviceCidr: 10.230.0.0/16
      podCidr:
        cidr: 10.58.128.0/17
        public: 10.58.128.0/18
        private: 10.58.192.0/18
    - serviceCidr: 10.229.0.0/16
      podCidr:
        cidr: 10.59.128.0/17
        public: 10.59.128.0/18
        private: 10.59.192.0/18
    - serviceCidr: 10.228.0.0/16
      podCidr:
        cidr: 10.60.128.0/17
        public: 10.60.128.0/18
        private: 10.60.192.0/18
    - serviceCidr: 10.227.0.0/16
      podCidr:
        cidr: 10.61.128.0/17
        public: 10.61.128.0/18
        private: 10.61.192.0/18
    - serviceCidr: 10.226.0.0/16
      podCidr:
        cidr: 10.62.128.0/17
        public: 10.62.128.0/18
        private: 10.62.192.0/18
    - serviceCidr: 10.225.0.0/16
      podCidr:
        cidr: 10.63.128.0/17
        public: 10.63.128.0/18
        private: 10.63.192.0/18
    - serviceCidr: 10.224.0.0/16
      podCidr:
        cidr: 10.64.128.0/17
        public: 10.64.128.0/18
        private: 10.64.192.0/18
    - serviceCidr: 10.223.0.0/16
      podCidr:
        cidr: 10.65.128.0/17
        public: 10.65.128.0/18
        private: 10.65.192.0/18
    - serviceCidr: 10.222.0.0/16
      podCidr:
        cidr: 10.66.128.0/17
        public: 10.66.128.0/18
        private: 10.66.192.0/18
    - serviceCidr: 10.221.0.0/16
      podCidr:
        cidr: 10.67.128.0/17
        public: 10.67.128.0/18
        private: 10.67.192.0/18
    - serviceCidr: 10.220.0.0/16
      podCidr:
        cidr: 10.68.128.0/17
        public: 10.68.128.0/18
        private: 10.68.192.0/18
    - serviceCidr: 10.219.0.0/16
      podCidr:
        cidr: 10.69.128.0/17
        public: 10.69.128.0/18
        private: 10.69.192.0/18
    - serviceCidr: 10.218.0.0/16
      podCidr:
        cidr: 10.70.128.0/17
        public: 10.70.128.0/18
        private: 10.70.192.0/18
    - serviceCidr: 10.217.0.0/16
      podCidr:
        cidr: 10.71.128.0/17
        public: 10.71.128.0/18
        private: 10.71.192.0/18
    - serviceCidr: 10.216.0.0/16
      podCidr:
        cidr: 10.72.128.0/17
        public: 10.72.128.0/18
        private: 10.72.192.0/18
    - serviceCidr: 10.215.0.0/16
      podCidr:
        cidr: 10.73.128.0/17
        public: 10.73.128.0/18
        private: 10.73.192.0/18
    - serviceCidr: 10.214.0.0/16
      podCidr:
        cidr: 10.74.128.0/17
        public: 10.74.128.0/18
        private: 10.74.192.0/18
    - serviceCidr: 10.213.0.0/16
      podCidr:
        cidr: 10.75.128.0/17
        public: 10.75.128.0/18
        private: 10.75.192.0/18
    - serviceCidr: 10.212.0.0/16
      podCidr:
        cidr: 10.76.128.0/17
        public: 10.76.128.0/18
        private: 10.76.192.0/18
    - serviceCidr: 10.211.0.0/16
      podCidr:
        cidr: 10.77.128.0/17
        public: 10.77.128.0/18
        private: 10.77.192.0/18
    - serviceCidr: 10.210.0.0/16
      podCidr:
        cidr: 10.78.128.0/17
        public: 10.78.128.0/18
        private: 10.78.192.0/18
    - serviceCidr: 10.209.0.0/16
      podCidr:
        cidr: 10.79.128.0/17
        public: 10.79.128.0/18
        private: 10.79.192.0/18
    - serviceCidr: 10.208.0.0/16
      podCidr:
        cidr: 10.80.128.0/17
        public: 10.80.128.0/18
        private: 10.80.192.0/18
    - serviceCidr: 10.207.0.0/16
      podCidr:
        cidr: 10.81.128.0/17
        public: 10.81.128.0/18
        private: 10.81.192.0/18
  gcp: |
    - nodeCidr: 10.16.128.0/17
      podCidr:
        cidr: 172.16.0.0/16
    - nodeCidr: 10.17.128.0/17
      podCidr:
        cidr: 172.17.0.0/16
    - nodeCidr: 10.18.128.0/17
      podCidr:
        cidr: 172.18.0.0/16
    - nodeCidr: 10.19.128.0/17
      podCidr:
        cidr: 172.19.0.0/16
    - nodeCidr: 10.20.128.0/17
      podCidr:
        cidr: 172.20.0.0/16
    - nodeCidr: 10.21.128.0/17
      podCidr:
        cidr: 172.21.0.0/16
    - nodeCidr: 10.22.128.0/17
      podCidr:
        cidr: 172.22.0.0/16
    - nodeCidr: 10.23.128.0/17
      podCidr:
        cidr: 172.23.0.0/16
    - nodeCidr: 10.24.128.0/17
      podCidr:
        cidr: 172.24.0.0/16
    - nodeCidr: 10.25.128.0/17
      podCidr:
        cidr: 172.25.0.0/16
    - nodeCidr: 10.26.128.0/17
      podCidr:
        cidr: 172.26.0.0/16
    - nodeCidr: 10.27.128.0/17
      podCidr:
        cidr: 172.27.0.0/16
    - nodeCidr: 10.28.128.0/17
      podCidr:
        cidr: 172.28.0.0/16
    - nodeCidr: 10.29.128.0/17
      podCidr:
        cidr: 172.29.0.0/16
    - nodeCidr: 10.30.128.0/17
      podCidr:
        cidr: 172.30.0.0/16
    - nodeCidr: 10.31.128.0/17
      podCidr:
        cidr: 172.31.0.0/16
    - nodeCidr: 10.32.128.0/17
      podCidr:
        cidr: 172.32.0.0/16
  openstack: |
    - serviceCidr: 10.82.192.0/18
      podCidr:
        cidr: 10.82.64.0/18
    - serviceCidr: 10.83.192.0/18
      podCidr:
        cidr: 10.83.64.0/18
    - serviceCidr: 10.84.192.0/18
      podCidr:
        cidr: 10.84.64.0/18
    - serviceCidr: 10.85.192.0/18
      podCidr:
        cidr: 10.85.64.0/18
    - serviceCidr: 10.86.192.0/18
      podCidr:
        cidr: 10.86.64.0/18
    - serviceCidr: 10.87.192.0/18
      podCidr:
        cidr: 10.87.64.0/18
    - serviceCidr: 10.88.192.0/18
      podCidr:
        cidr: 10.88.64.0/18
    - serviceCidr: 10.89.192.0/18
      podCidr:
        cidr: 10.89.64.0/18
    - serviceCidr: 10.90.192.0/18
      podCidr:
        cidr: 10.90.64.0/18
    - serviceCidr: 10.91.192.0/18
      podCidr:
        cidr: 10.91.64.0/18
{{ end }}