apiVersion: apiextensions.crossplane.io/v1
kind: Composition
metadata:
  name: xmeshsetups.skycluster.io
spec:
  compositeTypeRef:
    apiVersion: skycluster.io/v1alpha1
    kind: XMeshSetup
  mode: Pipeline
  pipeline:
    - step: extra-resources
      functionRef:
        name: function-extra-resources
      input:
        apiVersion: extra-resources.fn.crossplane.io/v1beta1
        kind: Input
        spec:
          extraResources:
            - apiVersion: kubernetes.crossplane.io/v1alpha1
              kind: ProviderConfig
              into: ClusterK8SProviderConfigs
              type: Selector
              selector:
                minMatch: 0
                maxMatch: 400
                matchLabels:
                  - key: skycluster.io/managed-by
                    type: Value
                    value: skycluster
                  - key: skycluster.io/config-type
                    type: Value
                    value: k8s-connection-data
            - apiVersion: helm.crossplane.io/v1beta1
              kind: ProviderConfig
              into: ClusterHelmProviderConfigs
              type: Selector
              selector:
                minMatch: 0
                maxMatch: 400
                matchLabels:
                  - key: skycluster.io/managed-by
                    type: Value
                    value: skycluster
                  - key: skycluster.io/config-type
                    type: Value
                    value: helm-connection-data
    - step: resources
      functionRef:
        name: function-kcl
      input:
        apiVersion: krm.kcl.dev/v1alpha1
        kind: KCLInput
        metadata:
          name: basic
        spec:
          dependencies: |
            k8s = "1.32.4"
            provider-kubernetes = { git = "https://github.com/skycluster-project/kcl-modules",version = "0.0.1" }
            provider-helm = { git = "https://github.com/skycluster-project/kcl-modules",version = "0.0.1" }
            helper = { git = "https://github.com/skycluster-project/kcl-modules",version = "0.0.1" }
          source: |2-

            # XKubeMesh Setup

            # import json

            import helper.v1alpha1.main as helper
            import provider_kubernetes.v1alpha2 as k8sv1a2

            oxr = option("params").oxr # observed composite resource
            ocds = option("params")?.ocds # observed composed resources
            extra = option("params")?.extraResources
            # _dxr = option("params").dxr # desired composite resource
            # dcds = option("params").dcds # desired composed resources

            ctx = option("params")?.ctx
            assert ctx is not Undefined, "Context must be provided in the params"

            _extraRes = ctx["apiextensions.crossplane.io/extra-resources"]
            assert _extraRes is not Undefined, "Extra resources must be provided in the context"

            # These are secrets containing istio remote secrets for remote clusters
            # They will be generated by the Job later here
            _remoteSecrets = extra["clusterRemoteSecrets"] or Undefined

            _remoteK8SProviderCfgs = _extraRes["ClusterK8SProviderConfigs"] or Undefined
            assert _remoteK8SProviderCfgs is not Undefined, "ClusterK8SProviderConfigs resource must be provided in the extra resources"

            _remoteHelmProviderCfgs = _extraRes["ClusterHelmProviderConfigs"] or Undefined
            assert _remoteHelmProviderCfgs is not Undefined, "ClusterHelmProviderConfigs resource must be provided in the extra resources"

            assert oxr.spec?.localCluster?.podCidr, "Pod CIDR must be provided in the spec"
            assert oxr.spec?.localCluster?.serviceCidr, "Service CIDR must be provided in the spec"

            # Default (local) provider config
            _k8sMgmtClusterName = "skycluster-management"

            _clusterNames = oxr.spec?.clusterNames or []
            assert len(_clusterNames) > 0, "At least one cluster name must be provided in the spec.clusters"

            #
            # fetch referenced xkubes objects based on the cluster names (cloud-provider specific k8s)
            #
            _clusterNamesRef = [o?.Resource?.status?.clusterName for s in _clusterNames for k, obj in extra if k == s for o in obj]
            # since we have the local management cluster as part of the multi-cluster setup
            # we manually add the name of the local management cluster
            _clusterNamesRef += [_k8sMgmtClusterName]

            _remoteK8SKubeconfigs = {
              cn = {
                kubeconfig = p?.Resource?.data?["kubeconfig"]
              } for cn in _clusterNamesRef for p in (extra?["kconfigSecrets"] or []) \
                  if cn == p?.Resource?.metadata?.labels?["skycluster.io/cluster-name"]
            }

            _remoteK8SProviderCfgsMap = {
              cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteK8SProviderCfgs \
                  if cn == p.metadata?.labels?["skycluster.io/cluster-name"]
            }

            _remoteHelmProviderCfgsMap = {
              cn = p.metadata?.name for cn in _clusterNamesRef for p in _remoteHelmProviderCfgs \
                  if cn == p.metadata?.labels?["skycluster.io/cluster-name"]
            }
            assert len(_remoteK8SProviderCfgsMap) == len(_remoteHelmProviderCfgsMap), \
              "Remote K8S and Helm provider configs must exist for all clusters, found {} K8S and {} Helm configs."\
                .format(len(_remoteK8SProviderCfgsMap), len(_remoteHelmProviderCfgsMap))

            # # merge the two
            _remoteProviderCfgsMaps = {
              cn = {
                k8s = _remoteK8SProviderCfgsMap[cn]
                helm = _remoteHelmProviderCfgsMap[cn]
              } for cn in _remoteHelmProviderCfgsMap
            }

            # Retrieve xkubes pod and service cidr
            # will be used in the next step
            _xkubesRefData = {
              o?.Resource?.status?.clusterName = {
                podCidr = o?.Resource?.status?.podCidr
                serviceCidr = o?.Resource?.status?.serviceCidr
                platform = o?.Resource?.spec?.providerRef?.platform
              } for s in _clusterNames for k, obj in extra if k == s for o in obj
            } | {
              # merge with local management cluster's data
              k = {
                podCidr = oxr.spec?.localCluster?.podCidr
                serviceCidr = oxr.spec?.localCluster?.serviceCidr
              } for k in [_k8sMgmtClusterName]
            }

            _items = []

            _items += [
                # This is a dummy pod that is required to apply rp_filter workaround for GCP clusters
                # to work with Submariner
                _helper_rp_filter_daemonset(s) for s in _clusterNamesRef \
                  if _xkubesRefData[s]?.platform in ["gcp", "aws"]
            ] if _remoteK8SKubeconfigs else []

            #
            # Only for aws, ensure IP forwarding is enabled using aws cli command
            #
            _items += [
              _helper_src_dst_check_daemonset(s) for s in _clusterNamesRef \
                if _xkubesRefData[s]?.platform == "aws" and _remoteProviderCfgsMaps \
                  or ocds?["src-dst-check-daemon-{}".format(s)]
            ]
            _items += [
              _helper_sync_routes_daemonset(s) for s in _clusterNamesRef \
                if _xkubesRefData[s]?.platform == "aws" and _remoteProviderCfgsMaps \
                  or ocds?["sync-routes-daemon-{}".format(s)]
            ]

            # # ###################### dxr ######################

            dxr = {
              **option("params").dxr,
              # status = {
              #   clusters = [{ 
              #     nameRef = s
              #     status = ""
              #   } for s in _clusterNamesRef]
              #   # log = json.encode(_xkubesRefData)
              # }
            }

            extraItems = {
              apiVersion = "meta.krm.kcl.dev/v1alpha1"
              kind = "ExtraResources"
              requirements = {
                **{s = {
                    apiVersion: "skycluster.io/v1alpha1",
                    kind: "XKube",
                    matchName: s
                  } for s in _clusterNames if s != _k8sMgmtClusterName
                }
                **{"submarinerSecret" = {
                    apiVersion: "v1",
                    kind: "Secret",
                    matchLabels: {
                      "skycluster.io/managed-by": "skycluster",
                      "skycluster.io/config-type": "connection-secret"
                    }
                }}
                **{"kconfigSecrets" = {
                    apiVersion: "v1",
                    kind: "Secret",
                    matchLabels: {
                      "skycluster.io/managed-by": "skycluster",
                      "skycluster.io/secret-type": "k8s-connection-data"
                    }
                }}
                **{"clusterRemoteSecrets" = {
                  apiVersion: "v1",
                  kind: "Secret",
                  matchLabels: {
                    "skycluster.io/managed-by": "skycluster",
                    "skycluster.io/secret-type": "cluster-cacert"
                  }
                }}
              }
            }

            items = [*_items, dxr, extraItems]

            _helper_rp_filter_daemonset = lambda s {
              k8sv1a2.Object{
                metadata = {
                  labels = {"skycluster.io/pod-type": "rp-filter"}
                  annotations = helper._set_resource_name("rp-filter-{}".format(s))
                }
                spec = {
                  forProvider.manifest = {
                    apiVersion = "apps/v1"
                    kind = "DaemonSet"
                    metadata = {
                      name = "rp-filter-{}".format(s),
                      namespace = "kube-system",
                      labels = {
                        "component": "kube-apiserver"
                        "skycluster.io/pod-type": "rp-filter"
                      },
                    }
                    spec = {
                      selector.matchLabels = {
                        "name": "rp-filter-{}".format(s)
                      }
                      template = {
                        metadata.labels = {
                          "name": "rp-filter-{}".format(s)
                        }
                        spec = {
                          hostNetwork = True
                          restartPolicy = "Always"
                          containers = [{
                            name = "netshoot-hostmount"
                            image = "nicolaka/netshoot"
                            args = ["/bin/bash","-c","echo 2 > /proc/sys/net/ipv4/conf/all/rp_filter; sysctl net.ipv4.conf.all.rp_filter; tail -f /dev/null"]
                            stdin = True
                            stdinOnce = True
                            terminationMessagePath = "/dev/termination-log"
                            terminationMessagePolicy = "File"
                            tty = True
                            securityContext = {
                              allowPrivilegeEscalation = True
                              privileged = True
                              runAsUser = 0
                              capabilities = {
                                add = ["ALL"]
                              }
                            }
                            volumeMounts = [{
                              mountPath = "/host"
                              name = "host-slash"
                              readOnly = True
                            }]
                          }]
                          volumes = [{
                            hostPath = {
                              path = "/"
                              type = ""
                            }
                            name = "host-slash"
                          }]
                        }
                      }
                    }
                  }
                  providerConfigRef = {
                    name = _remoteProviderCfgsMaps[s]?.k8s
                  }
                }
              }
            }

            _helper_sync_routes_daemonset = lambda s {
              k8sv1a2.Object{
                metadata = {
                  labels = {"skycluster.io/pod-type": "sync-routes"}
                  annotations = helper._set_resource_name("sync-routes-{}".format(s))
                }
                spec = {
                  forProvider.manifest = {
                    apiVersion = "apps/v1"
                    kind = "DaemonSet"
                    metadata = {
                      name = "sync-routes-{}".format(s),
                      namespace = "kube-system",
                      labels = {
                        "skycluster.io/pod-type": "sync-routes"
                      },
                    }
                    spec = {
                      selector.matchLabels = {
                        "name": "sync-routes-{}".format(s)
                      }
                      template = {
                        metadata.labels = {
                          "name": "sync-routes-{}".format(s)
                        }
                        spec = {
                          hostNetwork = True
                          restartPolicy = "Always"
                          containers = [{
                            name = "netshoot-hostmount"
                            image = "nicolaka/netshoot"
                            args = ["/bin/bash","-c", _helper_sync_table_2_with_main]
                            stdin = True
                            stdinOnce = True
                            terminationMessagePath = "/dev/termination-log"
                            terminationMessagePolicy = "File"
                            tty = True
                            securityContext = {
                              allowPrivilegeEscalation = True
                              privileged = True
                              runAsUser = 0
                              capabilities = {
                                add = ["ALL"]
                              }
                            }
                            volumeMounts = [{
                              mountPath = "/host"
                              name = "host-slash"
                              readOnly = True
                            }]
                          }]
                          volumes = [{
                            hostPath = {
                              path = "/"
                              type = ""
                            }
                            name = "host-slash"
                          }]
                        }
                      }
                    }
                  }
                  providerConfigRef = {
                    name = _remoteProviderCfgsMaps[s]?.k8s
                  }
                }
              }
            }

            _helper_src_dst_check_daemonset = lambda s {
              k8sv1a2.Object{
                metadata = {
                  labels = {"skycluster.io/pod-type": "src-dst-check"}
                  annotations = helper._set_resource_name("src-dst-check-{}".format(s))
                }
                spec = {
                  forProvider.manifest = {
                    apiVersion = "apps/v1"
                    kind = "DaemonSet"
                    metadata = {
                      name = "src-dst-check-{}".format(s),
                      namespace = "kube-system",
                      labels = {
                        "skycluster.io/pod-type": "src-dst-check"
                      },
                    }
                    spec = {
                      selector.matchLabels = {
                        "name": "src-dst-check-{}".format(s)
                      }
                      template = {
                        metadata.labels = {
                          "name": "src-dst-check-{}".format(s)
                        }
                        spec = {
                          hostNetwork = True
                          restartPolicy = "Always"
                          containers = [{
                            name = "aws-cli"
                            image = "amazon/aws-cli:2.15.40"
                            command: ["/bin/bash"]
                            args: ["-c", _helper_script_src_dst_check]
                            stdin = True
                            stdinOnce = True
                            terminationMessagePath = "/dev/termination-log"
                            terminationMessagePolicy = "File"
                            tty = True
                            securityContext = {
                              allowPrivilegeEscalation = True
                              privileged = True
                              runAsUser = 0
                              capabilities = {
                                add = ["ALL"]
                              }
                            }
                            volumeMounts = [
                              {
                              mountPath = "/host"
                              name = "host-slash"
                              readOnly = True
                              }
                            ]
                          }]
                          volumes = [
                            {
                              hostPath = {
                                path = "/"
                                type = ""
                              }
                              name = "host-slash"
                            }
                          ]
                        }
                      }
                    }
                  }
                  providerConfigRef = {
                    name = _remoteProviderCfgsMaps[s]?.k8s
                  }
                }
              }
            }

            _helper_script_route_setup = """\
            #!/bin/sh
            set -eu

            apk add --no-cache iproute2 >/dev/null 2>&1 || true

            TABLE_NAME=skycluster
            TABLE_ID=200
            IFF=submariner
            PRIO=2000

            CIDR1=__POD_CIDR__
            CIDR2=__SVC_CIDR__

            # ensure table mapping exists in /etc/iproute2/rt_tables
            if ! grep -qE "^[[:space:]]*$TABLE_ID[[:space:]]+$TABLE_NAME$" /etc/iproute2/rt_tables; then
              echo "$TABLE_ID $TABLE_NAME" >> /etc/iproute2/rt_tables
            fi

            # ensure rule for each CIDR
            for cidr in "$CIDR1" "$CIDR2"; do
              if ! ip rule show | grep -q -E "iif $IFF.*to +$cidr.*lookup $TABLE_NAME|to +$cidr.*iif $IFF.*lookup $TABLE_NAME"; then
                ip rule add iif "$IFF" to "$cidr" lookup "$TABLE_NAME" priority "$PRIO" || true
              fi
            done

            # replicate main table (excluding default) into the skycluster table continuously
            while true; do
              ip route flush table "$TABLE_NAME" 2>/dev/null || true
              ip route show table main | grep -v '^default' | while IFS= read -r line; do
                [ -z "$line" ] && continue
                ip route add table "$TABLE_NAME" $line 2>/dev/null || true
              done
              sleep 10
            done
            """

            _helper_script_src_dst_check = """\
            #!/bin/sh
            set -e

            TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" \
              -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" --connect-timeout 5 -m 5)
            if [ -z "$TOKEN" ]; then
              echo "Failed to get AWS metadata token"
              exit 1
            fi

            # use token to get instance-id
            INSTANCE_ID=$(curl -s -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/instance-id)
            if [ -z "$INSTANCE_ID" ]; then
              echo "Failed to get instance ID"
              exit 1
            fi

            echo "Instance ID: $INSTANCE_ID"

            while true; do
              ENI_IDS=$(aws ec2 describe-instances \
                --instance-ids "$INSTANCE_ID" \
                --query "Reservations[].Instances[].NetworkInterfaces[].NetworkInterfaceId" \
                --output text)

              for eni in $ENI_IDS; do
                echo "Disabling source-dest-check on ENI: $eni"
                aws ec2 modify-network-interface-attribute \
                  --network-interface-id "$eni" \
                  --no-source-dest-check || true
              done

              echo "Iteration complete. Sleeping 60 seconds..."
              sleep 60
            done
            """

            _helper_sync_table_2_with_main = """\
            #!/usr/bin/env bash
            set -euo pipefail

            TABLE_MAIN=main
            TABLE_TARGET=2
            MATCH_REGEX='vx-submariner|via 240\.'
            SLEEP=10

            log() {
              echo "$(date -Is) $*"
            }

            sync_routes() {
              tmp_main=$(mktemp)
              tmp_target=$(mktemp)
              tmp_main_prefixes=$(mktemp)
              tmp_target_prefixes=$(mktemp)

              ip -4 route show table $TABLE_MAIN | grep -E "$MATCH_REGEX" > $tmp_main 2>/dev/null || true
              awk '{print $1}' $tmp_main > $tmp_main_prefixes

              while IFS= read -r line || [ -n "$line" ]; do
                if [ -z "$line" ]; then
                  continue
                fi

                log "Processing: $line"

                # Primary sanitize: remove "proto static"
                sanitized=$(printf '%s\n' "$line" | sed -E 's/ proto static//g')

                log "Trying: ip route replace $sanitized table $TABLE_TARGET"
                if ip route replace $sanitized table $TABLE_TARGET 2>/dev/null; then
                  continue
                fi

                # Fallback sanitize: remove other kernel tokens that break "ip route replace"
                sanitized2=$(printf '%s\n' "$sanitized" | sed -E 's/ proto kernel//g; s/ scope link//g; s/ src [0-9]+\.[0-9]+\.[0-9]+\.[0-9]+//g')

                log "Trying: ip route replace $sanitized2 table $TABLE_TARGET"
                if ip route replace $sanitized2 table $TABLE_TARGET 2>/dev/null; then
                  continue
                fi

                log "ip route replace failed for lines: original='$line' sanitized='$sanitized' fallback='$sanitized2'"
              done < $tmp_main

              # Cleanup stale entries from target table
              ip -4 route show table $TABLE_TARGET | grep -E "$MATCH_REGEX" > $tmp_target 2>/dev/null || true
              awk '{print $1}' $tmp_target > $tmp_target_prefixes

              while IFS= read -r tprefix || [ -n "$tprefix" ]; do
                if [ -z "$tprefix" ]; then
                  continue
                fi
                if ! grep -Fxq "$tprefix" $tmp_main_prefixes; then
                  log "Deleting stale route from table $TABLE_TARGET: $tprefix"
                  ip route del "$tprefix" table $TABLE_TARGET 2>/dev/null || log "ip route del failed for: $tprefix"
                fi
              done < $tmp_target_prefixes

              rm -f $tmp_main $tmp_target $tmp_main_prefixes $tmp_target_prefixes
            }

            log "Starting submariner route sync (main -> table $TABLE_TARGET), interval $SLEEP"
            while true; do
              if ! sync_routes; then
                log "sync error"
              fi
              sleep $SLEEP
            done
            """

    - step: function-auto-ready
      functionRef:
        name: function-auto-ready
